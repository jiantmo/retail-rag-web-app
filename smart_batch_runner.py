#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹å¤„ç†ç‰ˆæœ¬çš„ multi_thread_runner
ä¸“é—¨ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è®¾è®¡ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè‡ªåŠ¨ token åˆ·æ–°
"""

import json
import time
import sys
import os
from pathlib import Path
from datetime import datetime
from multi_thread_runner import (
    DataverseSearchClient, 
    QuestionExtractor, 
    ProgressTracker,
    get_system_hardware_config,
    calculate_optimal_thread_config,
    process_single_question
)
from concurrent.futures import ThreadPoolExecutor, as_completed

class SmartBatchProcessor:
    """æ™ºèƒ½æ‰¹å¤„ç†å™¨ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†"""
    
    def __init__(self, batch_size=1000, checkpoint_interval=300):
        self.batch_size = batch_size  # æ¯æ‰¹å¤„ç†çš„é—®é¢˜æ•°é‡
        self.checkpoint_interval = checkpoint_interval  # ä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”ï¼ˆç§’ï¼‰
        self.client = DataverseSearchClient()
        
    def load_checkpoint(self, checkpoint_file):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint = json.load(f)
                print(f"ğŸ“‹ Loaded checkpoint: {checkpoint['completed_questions']} questions completed")
                return checkpoint
            except Exception as e:
                print(f"âš ï¸ Error loading checkpoint: {e}")
        return {"completed_questions": 0, "start_time": datetime.now().isoformat()}
    
    def save_checkpoint(self, checkpoint_file, completed_questions, start_time):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                "completed_questions": completed_questions,
                "start_time": start_time,
                "checkpoint_time": datetime.now().isoformat()
            }
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, indent=2)
            print(f"ğŸ’¾ Checkpoint saved: {completed_questions} questions completed")
        except Exception as e:
            print(f"âŒ Error saving checkpoint: {e}")
    
    def process_batch(self, questions_batch, progress_tracker, workers=30, delay=0.01):
        """å¤„ç†ä¸€æ‰¹é—®é¢˜"""
        print(f"ğŸ”„ Processing batch of {len(questions_batch)} questions with {workers} workers...")
        
        # æ£€æŸ¥ token çŠ¶æ€
        if not self.client._check_and_refresh_token_if_needed():
            print("âŒ Token refresh failed, skipping this batch")
            return False
        
        batch_start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(process_single_question, self.client, q, progress_tracker, delay)
                for q in questions_batch
            ]
            
            completed = 0
            for future in as_completed(futures):
                try:
                    future.result()
                    completed += 1
                except Exception as e:
                    print(f"âŒ Task failed: {e}")
                    completed += 1
        
        batch_time = time.time() - batch_start_time
        rate = len(questions_batch) / batch_time if batch_time > 0 else 0
        print(f"âœ… Batch completed in {batch_time:.1f}s, rate: {rate:.1f} questions/second")
        
        return True
    
    def process_all_questions(self, questions, workers=30, delay=0.01):
        """å¤„ç†æ‰€æœ‰é—®é¢˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        
        total_questions = len(questions)
        print(f"ğŸ¯ Processing {total_questions} questions in batches of {self.batch_size}")
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"smart_batch_results_{timestamp}.json"
        checkpoint_file = f"checkpoint_{timestamp}.json"
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self.load_checkpoint(checkpoint_file)
        start_index = checkpoint["completed_questions"]
        start_time = checkpoint["start_time"]
        
        if start_index > 0:
            print(f"ğŸ”„ Resuming from question {start_index + 1}")
        
        # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
        progress_tracker = ProgressTracker(total_questions, output_file)
        progress_tracker.completed = start_index
        
        # è®¡ç®—å‰©ä½™é—®é¢˜
        remaining_questions = questions[start_index:]
        last_checkpoint_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(remaining_questions), self.batch_size):
            batch = remaining_questions[i:i + self.batch_size]
            current_index = start_index + i
            
            print(f"\nğŸ“¦ Batch {(i // self.batch_size) + 1}: Questions {current_index + 1}-{current_index + len(batch)}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            success = self.process_batch(batch, progress_tracker, workers, delay)
            
            if not success:
                print("âŒ Batch processing failed, stopping...")
                break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            current_time = time.time()
            if current_time - last_checkpoint_time >= self.checkpoint_interval:
                self.save_checkpoint(checkpoint_file, current_index + len(batch), start_time)
                last_checkpoint_time = current_time
        
        # æœ€ç»ˆä¿å­˜
        progress_tracker.finalize_output()
        self.save_checkpoint(checkpoint_file, total_questions, start_time)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = progress_tracker.get_statistics()
        print("\n" + "=" * 80)
        print("ğŸ SMART BATCH PROCESSING COMPLETED!")
        print("=" * 80)
        print(f"ğŸ“Š Results: {stats['successful_requests']} successful, {stats['failed_requests']} failed")
        print(f"ğŸ“ˆ Success Rate: {stats['success_rate_percentage']:.1f}%")
        print(f"âš¡ Processing Rate: {stats['processing_rate_per_second']:.2f} questions/second")
        print(f"â³ Average Response Time: {stats['average_response_time_seconds']:.3f} seconds")
        print(f"ğŸ’¾ Results saved to: {output_file}")
        print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Smart Batch Multi-threaded Dataverse Search')
    parser.add_argument('--workers', '-w', type=int, default=30, help='Number of worker threads')
    parser.add_argument('--delay', '-d', type=float, default=0.01, help='Delay between requests in seconds')
    parser.add_argument('--batch-size', '-b', type=int, default=1000, help='Questions per batch')
    parser.add_argument('--checkpoint-interval', '-c', type=int, default=300, help='Checkpoint save interval in seconds')
    parser.add_argument('--path', '-p', default='.', help='Base path to search for files')
    
    args = parser.parse_args()
    
    # è·å–ç³»ç»Ÿé…ç½®
    print("ğŸ” Analyzing system hardware configuration...")
    system_config = get_system_hardware_config()
    
    print("ğŸ’» System Configuration:")
    print(f"   CPU: {system_config['cpu_logical_cores']} logical cores, {system_config['cpu_physical_cores']} physical cores")
    print(f"   Memory: {system_config['memory_total_gb']}GB total, {system_config['memory_available_gb']}GB available ({system_config['memory_usage_percent']:.1f}% used)")
    print(f"   Platform: {system_config['platform']} {system_config['platform_release']}")
    
    # æå–é—®é¢˜
    print("\nğŸ“ Finding question files...")
    files = QuestionExtractor.find_question_files(args.path)
    print(f"   Found {len(files)} files")
    
    print("ğŸ“ Extracting questions...")
    all_questions = []
    for file_path in files:
        questions = QuestionExtractor.extract_questions_from_file(file_path)
        all_questions.extend(questions)
        print(f"   {file_path.name}: {len(questions)} questions")
    
    if not all_questions:
        print("âŒ No questions found!")
        return
    
    print(f"\nğŸ¯ Total questions to process: {len(all_questions)}")
    print(f"âš™ï¸ Configuration:")
    print(f"   Workers: {args.workers}")
    print(f"   Delay: {args.delay}s")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Checkpoint interval: {args.checkpoint_interval}s")
    
    # åˆ›å»ºæ™ºèƒ½æ‰¹å¤„ç†å™¨
    processor = SmartBatchProcessor(
        batch_size=args.batch_size,
        checkpoint_interval=args.checkpoint_interval
    )
    
    # å¼€å§‹å¤„ç†
    processor.process_all_questions(
        all_questions,
        workers=args.workers,
        delay=args.delay
    )

if __name__ == "__main__":
    main()
